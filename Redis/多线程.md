## 版本
+ Github:redis/redis 
+ 分支:6.0 
+ 版本号:6.0.7

## 序
redis的多线程是把io操作分离出主线程事件循环,实际的命令操作依然是单(主)线程执行事件循环;

## 数据结构
```c
// src/server.h
struct redisServer {
    list *clients;              /* List of active clients */
    list *clients_to_close;     /* Clients to close asynchronously */
    list *clients_pending_write; // 准备分发给io线程处理的可写事件内容
    list *clients_pending_read;  // 放准备分发给io线程区处理的可读事件内容
}
```
```c
// src/networking.c
list *io_threads_list[IO_THREADS_MAX_NUM]; 分给每个io线程去处理的任务队列

_Atomic unsigned long io_threads_pending[IO_THREADS_MAX_NUM]; // 每个io线程当前pending等待处理的事件数(应该是防止每次都要遍历list算这个数量)

```
## 初始化
```c
// src/server.c
void InitServerLast() {
    // ...
    initThreadedIO();
    // ...
}
```
```c
// src/networking.c
void initThreadedIO(void) {
    // ...

    for (int i = 0; i < server.io_threads_num; i++) {
        // 每一个io线程对应一个任务list
        io_threads_list[i] = listCreate();

        /* Things we do only for the additional threads. */
        pthread_t tid;
        pthread_mutex_init(&io_threads_mutex[i],NULL);
        io_threads_pending[i] = 0;
        pthread_mutex_lock(&io_threads_mutex[i]); /* Thread will be stopped. */
        // #ref(IOThreadMain)
        if (pthread_create(&tid,NULL,IOThreadMain,(void*)(long)i) != 0) {
            // ...
            exit(1);
        }
        io_threads[i] = tid;
    }
}
```

## 主线程事件循环收到一个可读事件的处理
### 将有可读事件的连接添加到clients_pending_read队列里
```c
// src/networking.c
void readQueryFromClient(connection *conn) {
    client *c = connGetPrivateData(conn);
    int nread, readlen;
    size_t qblen;

    // 开启了多线程io后,主线程的可读事件处理加了一层异步逻辑,
    // 先把有可读事件的连接转到一个队列里,然后在主循环的beforesleep阶段把这些可读事件转给io线程处理
    if (postponeClientRead(c)) return;
    // ...
}

int postponeClientRead(client *c) {
    if (server.io_threads_active &&
        server.io_threads_do_reads &&
        !ProcessingEventsWhileBlocked &&
        !(c->flags & (CLIENT_MASTER|CLIENT_SLAVE|CLIENT_PENDING_READ)))
    {
        c->flags |= CLIENT_PENDING_READ; //设置flag为CLIENT_PENDING_READ,下次io线程走到同一个流程时这个函数就会返回0,因而接着执行原本的可读事件流程
        listAddNodeHead(server.clients_pending_read,c); // 将有可读事件的客户连接添加到clients_pending_read队列里,然后在主循环的beforesleep阶段分发给具体的io线程处理
        return 1;
    } else {
        return 0;
    }
}
```

### 主循环的beforesleep阶段分发clients_pending_read队列里的内容到io线程处理
```c
void beforeSleep(struct aeEventLoop *eventLoop) {
    // ...
    handleClientsWithPendingReadsUsingThreads();
    // ...
}

```
```c
// src/networking.c
int handleClientsWithPendingReadsUsingThreads(void) {

    /* Distribute the clients across N different lists. */
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_read,&li);
    int item_id = 0;
    while((ln = listNext(&li))) { // 将pending读事件里的任务平均分配给io线程
        client *c = listNodeValue(ln);
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }

    // 纪录每个io线程pending处理的事件数
    io_threads_op = IO_THREADS_OP_READ; //设置ip线程的状态为`read`
    for (int j = 1; j < server.io_threads_num; j++) {
        int count = listLength(io_threads_list[j]);
        io_threads_pending[j] = count;
    }

    // 等待所有io线程处理完这次分配的任务才继续
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += io_threads_pending[j];
        if (pending == 0) break;
    }


    // 到这里,整个clients_pending_read队列里的可读事件已经由io线程们处理好了,可以开始正式的处理 processInputBuffer
    while(listLength(server.clients_pending_read)) {
        // ...
        processInputBuffer(c);
    }

    return processed;
}
```

### io子线程处理可读事件
```c
void *IOThreadMain(void *myid) {
    while(1) { // 
        listIter li;
        listNode *ln;
        listRewind(io_threads_list[id],&li);
        while((ln = listNext(&li))) { 遍历所有自己的任务
            client *c = listNodeValue(ln);
            if (io_threads_op == IO_THREADS_OP_WRITE) {
                writeToClient(c,0); // 写信息到内核层
            } else if (io_threads_op == IO_THREADS_OP_READ) {
                readQueryFromClient(c->conn); // 从内核层读信息
            } else {
                serverPanic("io_threads_op value is unknown");
            }
        }
        listEmpty(io_threads_list[id]);
        io_threads_pending[id] = 0;
    }
}

```

### readQueryFromClient
```c
void readQueryFromClient(connection *conn) {
    client *c = connGetPrivateData(conn);
    int nread, readlen;
    size_t qblen;
    //...
    qblen = sdslen(c->querybuf);
    if (c->querybuf_peak < qblen) c->querybuf_peak = qblen;
    c->querybuf = sdsMakeRoomFor(c->querybuf, readlen);
    nread = connRead(c->conn, c->querybuf+qblen, readlen); // 通过系统调用将数据从内核层读到用户层,这个调用在微观上是比较耗时的,多线程io就是优化在了这个地方,使主线程不用调用耗时的系统函数

    // ...

    // 这里io线程在里面会判断连接的flags为CLIENT_PENDING_COMMAND而直接break出来,没有实际处理
    processInputBuffer(c);
}
void processInputBuffer(client *c) {
    /* Keep processing while there is something in the input buffer */
    while(c->qb_pos < sdslen(c->querybuf)) {
        // ...
        if (c->flags & CLIENT_PENDING_COMMAND) break;
        // ...
    }
}
```
### 主线程处理已经被io线程读到用户层的数据
```c
// src/networking.c
int handleClientsWithPendingReadsUsingThreads(void) {
    // ...
    while(listLength(server.clients_pending_read)) {
        // ...
        // 处理已经在用户层的数据
        processInputBuffer(c);
    }
    // ...

}
```

## 主线程事件循环怎么把数据发回客户端
### addReply 把有需要返回到客户端数据的连接加入到 server.clients_pending_write 队列中
```c
// src/networking.c
void addReply(client *c, robj *obj) {
    if (prepareClientToWrite(c) != C_OK) return;
    // ...
}
int prepareClientToWrite(client *c) {
    // ...
    if (!clientHasPendingReplies(c)) clientInstallWriteHandler(c);
    // ...
}
void clientInstallWriteHandler(client *c) {
    if (!(c->flags & CLIENT_PENDING_WRITE) &&
        (c->replstate == REPL_STATE_NONE ||
         (c->replstate == SLAVE_STATE_ONLINE && !c->repl_put_online_on_ack)))
    {
        c->flags |= CLIENT_PENDING_WRITE; // 设置连接标记
        listAddNodeHead(server.clients_pending_write,c); // 将连接加入到pending write队列中
    }
}
```
### beforeSleep调用handleClientsWithPendingWritesUsingThreads把写数据到内核层的任务分发给io线程们
```c
// src/server.c
void beforeSleep(struct aeEventLoop *eventLoop) {
    // ...
    handleClientsWithPendingWritesUsingThreads();
    // ...
}
```
```c
// src/networking.c
int handleClientsWithPendingWritesUsingThreads(void) {
    // ...
    listIter li;
    listNode *ln;
    listRewind(server.clients_pending_write,&li);
    int item_id = 0;
    while((ln = listNext(&li))) { // 将有pending_write的连接队列里的信息分发给io线程们
        client *c = listNodeValue(ln);
        c->flags &= ~CLIENT_PENDING_WRITE;
        int target_id = item_id % server.io_threads_num;
        listAddNodeTail(io_threads_list[target_id],c);
        item_id++;
    }


    // 等待io线程们完成
    while(1) {
        unsigned long pending = 0;
        for (int j = 1; j < server.io_threads_num; j++)
            pending += io_threads_pending[j];
        if (pending == 0) break;
    }

    // ...
}

```

### io线程写信息到内核层
```c
// src/networking.c
void *IOThreadMain(void *myid) {
    while(1) { // 
        listIter li;
        listNode *ln;
        listRewind(io_threads_list[id],&li);
        while((ln = listNext(&li))) { 遍历所有自己的任务
            client *c = listNodeValue(ln);
            if (io_threads_op == IO_THREADS_OP_WRITE) {
                writeToClient(c,0); // 写信息到内核层
            } else if (io_threads_op == IO_THREADS_OP_READ) {
                readQueryFromClient(c->conn); // 从内核层读信息
            } else {
                serverPanic("io_threads_op value is unknown");
            }
        }
        listEmpty(io_threads_list[id]);
        io_threads_pending[id] = 0;
    }
}

int writeToClient(client *c, int handler_installed) {
    /* Update total number of writes on server */
    server.stat_total_writes_processed++;

    ssize_t nwritten = 0, totwritten = 0;
    size_t objlen;
    clientReplyBlock *o;

    while(clientHasPendingReplies(c)) {
        if (c->bufpos > 0) {
            // connWrite就是系统调用将数据从用户层写到内核层(内核层再发送给客户端)
            nwritten = connWrite(c->conn,c->buf+c->sentlen,c->bufpos-c->sentlen);
        }
    }
    return C_OK;
}

```